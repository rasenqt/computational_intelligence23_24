{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random,numpy\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from game import Game, Move, RandomPlayer, MyPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuixoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5*5, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 44),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "## Constants definition\n",
    "num_iterations = 500\n",
    "num_matches = 10\n",
    "max_dim_replay_buff = 10_000\n",
    "time_to_update = 100\n",
    "gamma = 0.1\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size        \n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add_experience(self, experience):        \n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(experience)        \n",
    "        else:\n",
    "            self.buffer[self.position] = experience        \n",
    "            self.position = (self.position + 1) % self.buffer_size\n",
    "            \n",
    "    def sample_batch(self, batch_size):\n",
    "        batch_indices = np.random.choice(len(self.buffer), batch_size, replace=True)\n",
    "        batch = [self.buffer[i] for i in batch_indices]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Player\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def training(player1: 'Player', player2: 'Player'):\n",
    "    loss_tot = []\n",
    "    flag=1\n",
    "    replay_buff = ReplayBuffer(max_dim_replay_buff) # replay buffer, from which we sample for BATCH learning\n",
    "    # inizializza i modelli -> passali per parametro\n",
    "    \n",
    "    for step in tqdm(range(num_iterations), desc=\"Training Iterations\"):  \n",
    "        # we fill the replay buffer with experiences made with matches\n",
    "        for match in range(num_matches):\n",
    "            # lo uso solo in inferenza\n",
    "            # gioca una intera partita qui dentro e per ogni mossa, metti l'experience dentro il ReplayBuffer\n",
    "            # ...\n",
    "            # ... qui descrivere come viene effettivamente usata la rete neurale (magari impacchettarla in qualche funzione)\n",
    "            winner = -1\n",
    "            g = Game()\n",
    "            #player1 = MyPlayer()\n",
    "            #player2 = RandomPlayer()\n",
    "            players = [player1, player2]\n",
    "            \n",
    "            while winner < 0:\n",
    "                g.current_player_idx += 1\n",
    "                g.current_player_idx %= len(players)\n",
    "                #print(g.current_player_idx)\n",
    "                prev_state=deepcopy(g)\n",
    "                from_pos, slide = players[g.current_player_idx].make_move(g)\n",
    "                   \n",
    "                if g.current_player_idx==0:\n",
    "                    ##l'azione è il valore del q-value quindi un numero? o la posizione in questo caso  \n",
    "                    ##in quel caso replay_row=(prev_state.get_board,GeneratorNet((prev_state.get_board),g.get_board,reward)\n",
    "                    reward=g.compute_reward(from_pos, slide)\n",
    "                    replay_row=(prev_state.get_board, player1.last_action_value, deepcopy(g), reward)\n",
    "                    replay_buff.add_experience(replay_row)\n",
    "                    \n",
    "                if g.check_winner() != -1:\n",
    "                    break\n",
    "            \n",
    "            # replay_buffer.push(...) per ogni transizione di stato osservata ()\n",
    "            # restituisci il risultato sotto forma di tupla (st, a, st+1, r) - (State, action, next_state, reward)\n",
    "            \n",
    "        # Now we sample a batch of data from the ReplayBuffer in order to train the Agent\n",
    "        batch_to_train = replay_buff.sample_batch(50)\n",
    "      \n",
    "        TargetNet_targets = []\n",
    "        GeneratorNet_outputs = []\n",
    "        player1.myplayer_zero_grad()\n",
    "        # Now we need to compute the list of targets (made with the TargetNet) and the one with the Q values for the \"current\" state\n",
    "        \n",
    "        for element in batch_to_train:\n",
    "            _, action_val, new_state, reward = element\n",
    "            \n",
    "            # per ogni tupla di 4 elementi prendere l'action (a) (sarà un valore...q value)\n",
    "            # Rappresenta il primo termine della Loss function (MSE) => output\n",
    "            # inserirlo dentro GeneratorNet_outputs\n",
    "            GeneratorNet_outputs.append(action_val)\n",
    "\n",
    "            # Per calcolare il secondo termine, devo prendere la reward dalla tupla, gamma dai parametri (vedi sopra) e il q value dell'azione migliore dello stato successivo\n",
    "            # Rappresenta il secondo termine della Loss function (MSE) => target\n",
    "            # Per calcolare il target, dare in pasto alla NN lo stato+1 contenuto nella tupla element\n",
    "            # inserirlo dentro TargetNet_targets\n",
    "            max_action_newstate = player1.compute_target(new_state)\n",
    "         \n",
    "            res = reward + gamma*max_action_newstate\n",
    "            TargetNet_targets.append(res)\n",
    "        GeneratorNet_outputs =torch.tensor(GeneratorNet_outputs, dtype=torch.float32,requires_grad=True).to(player1.device)\n",
    "        TargetNet_targets = torch.tensor(TargetNet_targets, dtype=torch.float32,requires_grad=True).to(player1.device)\n",
    "       \n",
    "        \n",
    "        if flag==1:\n",
    "          \n",
    "            # print(player1.GeneratorNet.state_dict())  \n",
    "             print(player1.TargetNet.state_dict()) \n",
    "             flag=0\n",
    "        loss_curr = player1.myplayer_loss_and_update_params(GeneratorNet_outputs, TargetNet_targets) \n",
    "       \n",
    "\n",
    "        loss_tot.append(loss_curr)\n",
    "        \n",
    "        if (step % time_to_update) == 0:\n",
    "            # update the parameter of the TargetNet\n",
    "            player1.copy_params_TargetNet()\n",
    "            \n",
    "       \n",
    "        #printa come varia \n",
    "    print(\"suca\")\n",
    "    #print(player1.GeneratorNet.state_dict())        \n",
    "    print(player1.TargetNet.state_dict()) \n",
    "    return player1.GeneratorNet.state_dict()\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Iterations:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear_relu_stack.0.weight', tensor([[-0.0407,  0.0842, -0.1713,  ...,  0.0183,  0.1666,  0.1921],\n",
      "        [-0.1550,  0.0843, -0.1830,  ..., -0.0606,  0.0536,  0.1799],\n",
      "        [-0.1144, -0.1517, -0.1941,  ...,  0.1954,  0.1809, -0.0376],\n",
      "        ...,\n",
      "        [ 0.0420, -0.0828,  0.1090,  ..., -0.0485,  0.1563, -0.1397],\n",
      "        [-0.0472, -0.1840,  0.1153,  ..., -0.1894,  0.1209,  0.1178],\n",
      "        [ 0.1365,  0.0763, -0.1559,  ...,  0.0941, -0.0328,  0.1177]])), ('linear_relu_stack.0.bias', tensor([-0.0854,  0.1289,  0.0651, -0.0808,  0.1080,  0.1945,  0.1485,  0.0887,\n",
      "        -0.1669, -0.0744,  0.1381, -0.0841, -0.0822, -0.1427, -0.1954,  0.1152,\n",
      "        -0.1179, -0.1415,  0.0290, -0.1054, -0.1948,  0.0588,  0.1986,  0.0526,\n",
      "        -0.1941, -0.0325, -0.0437,  0.1245, -0.0415, -0.1567, -0.1098, -0.0935,\n",
      "        -0.0637, -0.0373, -0.1815, -0.0030, -0.1112, -0.1717,  0.0632,  0.0011,\n",
      "         0.0066, -0.1318,  0.1561,  0.0235,  0.1410,  0.1182, -0.0191,  0.0809,\n",
      "         0.0637,  0.1238, -0.0884,  0.1909,  0.1618,  0.0347,  0.1752,  0.0800,\n",
      "         0.0775,  0.1078,  0.0351,  0.1272, -0.1877, -0.1964, -0.0788,  0.0863,\n",
      "        -0.1230, -0.1920, -0.0596, -0.1245, -0.1021, -0.1747, -0.1476,  0.0608,\n",
      "        -0.1788, -0.1584,  0.1675,  0.1006, -0.0267, -0.0793,  0.1211,  0.0628,\n",
      "        -0.1855, -0.1526, -0.0150,  0.1603,  0.0730, -0.0715, -0.0489,  0.0271,\n",
      "         0.1135,  0.0073,  0.1984, -0.1770, -0.0754, -0.0937, -0.0532, -0.1726,\n",
      "        -0.1623,  0.0624, -0.1096,  0.0658])), ('linear_relu_stack.2.weight', tensor([[-0.0127, -0.0698,  0.0227,  ..., -0.0191, -0.0922, -0.0307],\n",
      "        [-0.0765, -0.0919, -0.0468,  ..., -0.0652, -0.0384,  0.0801],\n",
      "        [ 0.0129,  0.0848, -0.0031,  ..., -0.0487, -0.0665, -0.0254],\n",
      "        ...,\n",
      "        [-0.0629, -0.0551,  0.0693,  ...,  0.0264, -0.0264,  0.0574],\n",
      "        [-0.0268,  0.0371, -0.0379,  ..., -0.0778, -0.0483, -0.0504],\n",
      "        [ 0.0672,  0.0566,  0.0810,  ..., -0.0622, -0.0644,  0.0678]])), ('linear_relu_stack.2.bias', tensor([ 0.0545,  0.0604, -0.0333, -0.0514,  0.0846,  0.0436, -0.0113,  0.0072,\n",
      "        -0.0009, -0.0082, -0.0326,  0.0375, -0.0316, -0.0788,  0.0602,  0.0928,\n",
      "         0.0147,  0.0248,  0.0350,  0.0883,  0.0870, -0.0657,  0.0324,  0.0868,\n",
      "        -0.0678,  0.0644, -0.0285,  0.0784, -0.0329, -0.0734, -0.0649,  0.0890,\n",
      "         0.0220, -0.0358,  0.0546, -0.0275,  0.0991, -0.0826, -0.0818, -0.0016,\n",
      "         0.0854,  0.0761, -0.0569, -0.0225, -0.0142,  0.0521,  0.0686, -0.0990,\n",
      "        -0.0073, -0.0210, -0.0044,  0.0851,  0.0724, -0.0428, -0.0056, -0.0153,\n",
      "        -0.0508, -0.0871,  0.0136,  0.0755, -0.0712,  0.0843, -0.0169, -0.0420,\n",
      "         0.0966,  0.0753, -0.0208,  0.0050,  0.0783, -0.0986,  0.0131, -0.0639,\n",
      "         0.0675, -0.0890,  0.0921, -0.0592, -0.0013, -0.0634, -0.0467,  0.0143,\n",
      "        -0.0553,  0.0743,  0.0802, -0.0089,  0.0590, -0.0646, -0.0564, -0.0211,\n",
      "         0.0715, -0.0527,  0.0653,  0.0636, -0.0212,  0.0498,  0.0790,  0.0659,\n",
      "        -0.0881, -0.0050, -0.0698, -0.0188])), ('linear_relu_stack.4.weight', tensor([[ 0.0763, -0.0010,  0.0391,  ...,  0.0169, -0.0812,  0.0221],\n",
      "        [-0.0127,  0.0495, -0.0275,  ...,  0.0386, -0.0367,  0.0669],\n",
      "        [-0.0463,  0.0425, -0.0207,  ...,  0.0170,  0.0091,  0.0068],\n",
      "        ...,\n",
      "        [ 0.0259, -0.0753, -0.0809,  ..., -0.0420, -0.0885, -0.0921],\n",
      "        [-0.0027, -0.0420, -0.0237,  ..., -0.0481, -0.0484,  0.0313],\n",
      "        [ 0.0422,  0.0940, -0.0951,  ...,  0.0830,  0.0648,  0.0403]])), ('linear_relu_stack.4.bias', tensor([ 0.0324, -0.0701, -0.0819, -0.0403,  0.0064, -0.0307,  0.0940, -0.0984,\n",
      "         0.0429,  0.0858,  0.0383, -0.0503,  0.0308, -0.0184,  0.0323, -0.0732,\n",
      "        -0.0226,  0.0114, -0.0613, -0.0358,  0.0203,  0.0410,  0.0464,  0.0919,\n",
      "        -0.0151,  0.0347,  0.0232,  0.0042,  0.0840, -0.0446,  0.0584,  0.0918,\n",
      "        -0.0911,  0.0123, -0.0960, -0.0450,  0.0515,  0.0776, -0.0651, -0.0503,\n",
      "        -0.0700, -0.0978, -0.0581, -0.0185]))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Iterations:  98%|█████████▊| 490/500 [00:31<00:00, 13.25it/s]"
     ]
    }
   ],
   "source": [
    "player1 = MyPlayer()\n",
    "player2 = RandomPlayer()\n",
    "trained_model_params = training(player1, player2)\n",
    "TrainedGeneratorNet = QuixoNet.load_state_dict(trained_model_params) # TRAINED NETWORK to use in INFERENCE PHASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = 0\n",
    "losts = 0\n",
    "draws = 0\n",
    "num_match_test = 1000\n",
    "\n",
    "for _ in num_match_test:\n",
    "    # play_match(TrainedGeneratedNet, random)\n",
    "    # increment wins/losts/draws \n",
    "    print(\"miao\")\n",
    "\n",
    "print(f\"Accuracy: {(wins/num_match_test)*100}\")\n",
    "print(\"Wins: {wins} - Losts: {losts} - Draws {draws}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
