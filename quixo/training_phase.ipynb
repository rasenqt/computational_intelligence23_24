{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random,numpy\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from game import Game, Move, RandomPlayer, MyPlayer,translate_number_to_position_direction,translate_number_to_position,TrainedPlayer\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "## Constants definition\n",
    "\n",
    "# These parameter have been obtained by running a genetic algorithm over individuals representing the network hyper parameters\n",
    "# Based on the fact that the agent could start as first/second, we might have different hyperparameters for the network. The \n",
    "# following, represents a possible base to start from (model version for second turn)\n",
    "\n",
    "num_iterations = 800\n",
    "num_matches = 6\n",
    "max_dim_replay_buff = 589\n",
    "time_to_update = 27\n",
    "gamma = 0.280\n",
    "batch_size= 409\n",
    "\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size        \n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add_experience(self, experience):        \n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(experience)        \n",
    "        else:\n",
    "            self.buffer[self.position] = experience        \n",
    "            self.position = (self.position + 1) % self.buffer_size\n",
    "            \n",
    "    def sample_batch(self, batch_size):\n",
    "        batch_indices = np.random.choice(len(self.buffer), batch_size, replace=True)\n",
    "        batch = [self.buffer[i] for i in batch_indices]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Player\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def training(agent_to_train: 'Player', opponent: 'Player', starts_first: bool):\n",
    "    loss_tot = []\n",
    "    taboo_set = set()\n",
    "\n",
    "    replay_buff = ReplayBuffer(max_dim_replay_buff) # replay buffer, from which we sample for BATCH learning\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    for step in tqdm(range(num_iterations), desc=\"Training Iterations\"):  \n",
    "        # we fill the replay buffer with experiences made with matches\n",
    "        for match in range(num_matches):\n",
    "            winner = -1\n",
    "            g = Game()\n",
    "            g.current_player_idx = int(starts_first) # If TRUE it is set to 1, there the agents starts as first \n",
    "            players = [agent_to_train, opponent] \n",
    "            go = False \n",
    "\n",
    "            while winner < 0:\n",
    "                    g.current_player_idx += 1\n",
    "                    g.current_player_idx %= len(players)\n",
    "                    prev_state=deepcopy(g)\n",
    "                    from_pos, slide = players[g.current_player_idx].make_move(g)\n",
    "                    g._Game__move(from_pos, slide, g.current_player_idx)\n",
    "                    \n",
    "                    if g.current_player_idx==0:\n",
    "                        reward=g.compute_reward()\n",
    "                        go = True\n",
    "                        \n",
    "                    elif go and g.current_player_idx==1: \n",
    "                        if (tuple(prev_state.get_flat_board()), agent_to_train.last_action_number, tuple(deepcopy(g).get_flat_board())) not in taboo_set :\n",
    "                            replay_row=(prev_state.get_flat_board(), agent_to_train.last_action_number, deepcopy(g), reward, reward==1)\n",
    "                            taboo_set.add((tuple(prev_state.get_flat_board()),agent_to_train.last_action_number, tuple(deepcopy(g).get_flat_board())))\n",
    "                            replay_buff.add_experience(replay_row)\n",
    "\n",
    "                    if g.check_winner() != -1:\n",
    "                        break\n",
    "            \n",
    "        batch_to_train = replay_buff.sample_batch(batch_size)\n",
    "      \n",
    "        #divide the batch\n",
    "        state_batch, action_num, next_state_batch, reward, done = zip(*batch_to_train) \n",
    "\n",
    "        #forward the Generator\n",
    "        q_values = agent_to_train.GeneratorNet(torch.tensor(state_batch, dtype=torch.float32)).to(agent_to_train.device)\n",
    "        q_values_target = torch.zeros(batch_size, 44).to(agent_to_train.device)\n",
    "\n",
    "        #update q_values target by using Bellman Equation  \n",
    "        for i in range(batch_size): \n",
    "            if done[i] == False:\n",
    "                q_values_target[i, action_num[i]] = reward[i] + gamma * torch.tensor(agent_to_train.compute_target(next_state_batch[i])).to(agent_to_train.device).item()\n",
    "            else:\n",
    "               q_values_target[i, action_num[i]] = reward[i] \n",
    "          \n",
    "        agent_to_train.optimizer.zero_grad()\n",
    "        loss_curr=agent_to_train.criterion(q_values,q_values_target).to(agent_to_train.device)\n",
    "        loss_curr.backward()\n",
    "    \n",
    "        agent_to_train.optimizer.step()\n",
    "\n",
    "        loss_tot.append(loss_curr)\n",
    "      \n",
    "        if (step % time_to_update) == 0:\n",
    "            # update the parameter of the TargetNet\n",
    "            agent_to_train.copy_params_TargetNet()\n",
    "            \n",
    "   \n",
    "    return agent_to_train.GeneratorNet.state_dict(), loss_tot\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Iterations:   0%|          | 0/300 [00:00<?, ?it/s]C:\\Users\\chito\\AppData\\Local\\Temp\\ipykernel_4316\\1211647654.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  q_values = agent_to_train.GeneratorNet(torch.tensor(state_batch, dtype=torch.float32)).to(agent_to_train.device)\n",
      "Training Iterations: 100%|██████████| 300/300 [00:50<00:00,  5.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_train_first = MyPlayer()\n",
    "agent_train_second = MyPlayer()\n",
    "opponent = RandomPlayer()\n",
    "\n",
    "TrainedGeneratorNet_first = TrainedPlayer()\n",
    "trained_model_params_first, loss_tot_first = training(agent_train_first, opponent, True) # this model is trained to start first\n",
    "TrainedGeneratorNet_first.GeneratorNet.load_state_dict(trained_model_params_first) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Iterations:  17%|█▋        | 52/300 [00:08<00:42,  5.90it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Change the following settings based on the genetic algorithm for the model version that start as second\n",
    "\n",
    "num_iterations = ...\n",
    "num_matches = ...\n",
    "max_dim_replay_buff = ...\n",
    "time_to_update = ...\n",
    "gamma = ...\n",
    "batch_size = ...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "TrainedGeneratorNet_second = TrainedPlayer()\n",
    "trained_model_params_second, loss_tot_second = training(agent_train_second, opponent, False) # this model is trained to start second\n",
    "TrainedGeneratorNet_second.GeneratorNet.load_state_dict(trained_model_params_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "vals_first = [el.detach().numpy() for el in loss_tot_first]\n",
    "vals_second = [el.detach().numpy() for el in loss_tot_second]\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, len(loss_tot_second) + 1)\n",
    "\n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, vals_first, label='Training Loss (first turn)')\n",
    "plt.plot(epochs, vals_second, label='Training Loss (second turn)')\n",
    "\n",
    "\n",
    "# Add title and axes labels\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Display the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(TrainedGeneratorNet_first.GeneratorNet.state_dict(), './trained_models/first_turn_model.pth')\n",
    "torch.save(TrainedGeneratorNet_second.GeneratorNet.state_dict(), './trained_models/second_turn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game, RandomPlayer, MyPlayer, TrainedPlayer, TrainedPlayer_Complete\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "TrainedGeneratorNet = TrainedPlayer()\n",
    "TrainedGeneratorNet.GeneratorNet.load_state_dict(torch.load('./trained_models/try_second.pth'))\n",
    "\n",
    "wins = 0\n",
    "losts = 0\n",
    "draws = 0\n",
    "num_match_test = 100\n",
    "\n",
    "for step in tqdm(range(num_match_test), desc=\"Game\"):\n",
    "    player=RandomPlayer()\n",
    "    g=Game()\n",
    "    #g.current_player_idx = step%2\n",
    "    #g.current_player_idx = 1\n",
    "    winner=g.play(TrainedGeneratorNet, player)\n",
    "    if winner==0:\n",
    "        wins+=1\n",
    "       \n",
    "    if winner==1:\n",
    "        losts+=1\n",
    "        \n",
    "    if winner ==-1:\n",
    "        draws+=1\n",
    "\n",
    "print(f\"Accuracy: {(wins/num_match_test)*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
