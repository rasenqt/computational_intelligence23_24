{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random, numpy\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from game import Game, Move, RandomPlayer, MyPlayer,translate_number_to_position_direction,translate_number_to_position,TrainedPlayer\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genome:\n",
    "    def __init__(self, ):\n",
    "        self.genotype = create_rand_gen()\n",
    "        self.fitness = float(\"-inf\")\n",
    "\n",
    "cell_max_values = [\n",
    "    (1, 8),    # num_matches\n",
    "    (100, 1_000),  # max_dim_replay_buff\n",
    "    (2, 80),   # time_to_update\n",
    "    (0.2, 1.0),   # gamma\n",
    "    (50, 500)    # batch_size\n",
    "]\n",
    "\n",
    "def create_rand_gen(): # creates the genome, referred to the scale of the values we'd like to estimate    \n",
    "    # Define the maximum values for each cell\n",
    "    # Generate a random vector\n",
    "    genome = [random.uniform(cell[0], cell[1]) if isinstance(cell[0], float) else random.randint(cell[0], cell[1]) for cell in cell_max_values]\n",
    "    genome[4] = min(genome[1], genome[4]) # max_dim_replay_buff > batch_size\n",
    "    return genome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:     \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size        \n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add_experience(self, experience):        \n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(experience)        \n",
    "        else:\n",
    "            self.buffer[self.position] = experience        \n",
    "            self.position = (self.position + 1) % self.buffer_size\n",
    "            \n",
    "    def sample_batch(self, batch_size):\n",
    "        batch_indices = np.random.choice(len(self.buffer), batch_size, replace=True)\n",
    "        batch = [self.buffer[i] for i in batch_indices]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Player\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def training(individual: 'Genome', starts_first: bool):\n",
    "    num_iterations = 100\n",
    "    num_matches = individual.genotype[0]\n",
    "    max_dim_replay_buff = individual.genotype[1]\n",
    "    time_to_update = individual.genotype[2]\n",
    "    gamma = individual.genotype[3]\n",
    "    batch_size = individual.genotype[4]\n",
    "    agent_to_train = MyPlayer()\n",
    "    opponent = RandomPlayer()\n",
    "\n",
    "    loss_tot = []\n",
    "    \n",
    "    taboo_set = set()\n",
    "    replay_buff = ReplayBuffer(max_dim_replay_buff) # replay buffer, from which we sample for BATCH learning\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    for step in range(num_iterations):  \n",
    "        # we fill the replay buffer with experiences made with matches\n",
    "        for match in range(num_matches):\n",
    "            winner = -1\n",
    "            g = Game()\n",
    "            g.current_player_idx = int(starts_first)\n",
    "            players = [agent_to_train, opponent]\n",
    "\n",
    "            go = False # needed in order to compute the next state\n",
    "            while winner < 0:\n",
    "                    g.current_player_idx += 1\n",
    "                    g.current_player_idx %= len(players)\n",
    "                    prev_state=deepcopy(g)\n",
    "                    from_pos, slide = players[g.current_player_idx].make_move(g)\n",
    "                    g._Game__move(from_pos, slide, g.current_player_idx)\n",
    "\n",
    "                    \n",
    "                    if g.current_player_idx==0:\n",
    "                        reward=g.compute_reward()\n",
    "                        go = True\n",
    "                        \n",
    "                    elif go and g.current_player_idx==1:\n",
    "                        if (tuple(prev_state.get_flat_board()), agent_to_train.last_action_number, tuple(deepcopy(g).get_flat_board())) not in taboo_set :\n",
    "                            replay_row=(prev_state.get_flat_board(), agent_to_train.last_action_number, deepcopy(g), reward, reward==1)\n",
    "                            taboo_set.add((tuple(prev_state.get_flat_board()),agent_to_train.last_action_number, tuple(deepcopy(g).get_flat_board())))\n",
    "                            replay_buff.add_experience(replay_row)\n",
    "\n",
    "                    if g.check_winner() != -1:\n",
    "                        break\n",
    "            \n",
    "        # Now we sample a batch of data from the ReplayBuffer in order to train the Agent\n",
    "        batch_to_train = replay_buff.sample_batch(batch_size)\n",
    "      \n",
    "        #divide the batch\n",
    "        state_batch, action_num, next_state_batch, reward, done = zip(*batch_to_train) \n",
    "\n",
    "        #forward the Generator\n",
    "        q_values = agent_to_train.GeneratorNet(torch.tensor(state_batch, dtype=torch.float32)).to(agent_to_train.device)\n",
    "        q_values_target = torch.zeros(batch_size, 44).to(agent_to_train.device)\n",
    "\n",
    "\n",
    "        #update q_values target by using Bellman Equation  \n",
    "        for i in range(batch_size): \n",
    "            if done[i] == False:\n",
    "                q_values_target[i, action_num[i]] = reward[i] + gamma * torch.tensor(agent_to_train.compute_target(next_state_batch[i])).to(agent_to_train.device).item()\n",
    "            else:\n",
    "               q_values_target[i, action_num[i]] = reward[i] \n",
    "          \n",
    "        agent_to_train.optimizer.zero_grad()\n",
    "        loss_curr=agent_to_train.criterion(q_values,q_values_target).to(agent_to_train.device)\n",
    "        loss_curr.backward()\n",
    "    \n",
    "        agent_to_train.optimizer.step()\n",
    "\n",
    "        loss_tot.append(loss_curr)\n",
    "      \n",
    "        if (step % time_to_update) == 0:\n",
    "            # update the parameter of the TargetNet\n",
    "            agent_to_train.copy_params_TargetNet()\n",
    "            \n",
    "    return  agent_to_train.GeneratorNet.state_dict()\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULATION_SIZE = 10\n",
    "OFFSPRING_SIZE = 4\n",
    "LOCI = 5\n",
    "MUTATION_PROBABILITY = 0.5\n",
    "BIT_MUTATION_PROBABILITY = 0.5\n",
    "NUM_GENERATION = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from copy import deepcopy\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def new_offspring(parent1: 'Genome', parent2: 'Genome') -> 'Genome':\n",
    "    if random.random() < MUTATION_PROBABILITY:\n",
    "        return mutate(parent1)\n",
    "    return three_cut_xover(parent1, parent2)\n",
    "    \n",
    "def mutate(parent: 'Genome') -> 'Genome':\n",
    "    new_offspring = deepcopy(parent)\n",
    "\n",
    "    for _ in range(LOCI):\n",
    "        if random.random() < BIT_MUTATION_PROBABILITY:\n",
    "            index_to_mutate = randint(0, LOCI-1)\n",
    "            new_offspring.genotype[index_to_mutate] = random.uniform(cell_max_values[index_to_mutate][0], cell_max_values[index_to_mutate][1]) if isinstance(cell_max_values[index_to_mutate][0], float) else random.randint(cell_max_values[index_to_mutate][0], cell_max_values[index_to_mutate][1])\n",
    "            new_offspring.genotype[4] = min(new_offspring.genotype[1], new_offspring.genotype[4]) # max_dim_replay_buff > batch_size\n",
    "    \n",
    "    return new_offspring\n",
    "\n",
    "def three_cut_xover(ind1: 'Genome', ind2: 'Genome') -> 'Genome':\n",
    "    one_cut_point = randint(0, int((LOCI)*0.3))\n",
    "    two_cut_point = randint(int((LOCI)*0.3), int((LOCI)*0.6))\n",
    "    three_cut_point = randint(int((LOCI)*0.6), LOCI - 1)\n",
    "  \n",
    "    # Order the cut points\n",
    "    cut_points = sorted([one_cut_point, two_cut_point, three_cut_point])\n",
    "    \n",
    "    new_ind = Genome()\n",
    "    new_ind.genotype = (ind1.genotype[:cut_points[0]] +\n",
    "                        ind2.genotype[cut_points[0]:cut_points[1]] +\n",
    "                        ind1.genotype[cut_points[1]:cut_points[2]] +\n",
    "                        ind2.genotype[cut_points[2]:])\n",
    "    \n",
    "    new_ind.genotype[4] = min(new_ind.genotype[1], new_ind.genotype[4]) \n",
    "    \n",
    "    assert len(new_ind.genotype) == LOCI\n",
    "    return new_ind\n",
    "\n",
    "def compute_fitness(individual: 'Genome', starts_first: bool):\n",
    "    TrainedGeneratorNet = TrainedPlayer()\n",
    "    trained_weights = training(individual, starts_first)\n",
    "    TrainedGeneratorNet.GeneratorNet.load_state_dict(trained_weights) # TRAINED NETWORK to use in INFERENCE PHASE\n",
    "\n",
    "    wins = 0\n",
    "    num_match_test = 100\n",
    "\n",
    "    for step in range(num_match_test):\n",
    "        player=RandomPlayer()\n",
    "        g=Game()\n",
    "        g.current_player_idx = int(starts_first)\n",
    "        winner=g.play(TrainedGeneratorNet, player)\n",
    "        \n",
    "        if winner==0:\n",
    "            wins+=1\n",
    "\n",
    "    individual.fitness = (wins/num_match_test)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def genetic_algorithm(starts_first: bool):\n",
    "        population = [Genome() for _ in range(POPULATION_SIZE)]\n",
    "        for i in range(OFFSPRING_SIZE):\n",
    "                compute_fitness(population[i], starts_first)\n",
    "\n",
    "        population.sort(key=lambda i: i.fitness, reverse=True)\n",
    "        best_fitness = population[0].fitness\n",
    "        best_individual = population[0]\n",
    "        gen = 0\n",
    "\n",
    "        while  gen < NUM_GENERATION:                \n",
    "                for _ in range(OFFSPRING_SIZE):\n",
    "                        offspring = new_offspring(population[0], population[1])\n",
    "                        compute_fitness(offspring, starts_first)\n",
    "\n",
    "                population.extend([offspring])\n",
    "                population.sort(key=lambda i: i.fitness, reverse=True)\n",
    "                population = population[:POPULATION_SIZE]\n",
    "                best_individual = population[0]\n",
    "                best_fitness = population[0].fitness\n",
    "                gen += 1\n",
    "                print(f\"Best individual (until now): {best_individual.genotype}\")\n",
    "                print(f\"Best fitness: {best_fitness}\")\n",
    "\n",
    "\n",
    "        print(f\"Best individual with fitness: {best_fitness}\")\n",
    "        \n",
    "        return best_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chito\\AppData\\Local\\Temp\\ipykernel_25764\\176035809.py:88: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  q_values = agent_to_train.GeneratorNet(torch.tensor(state_batch, dtype=torch.float32)).to(agent_to_train.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best individual (until now): [1, 364, 77, 0.8617795894984199, 350]\n",
      "Best fitness: 71.0\n",
      "Best individual (until now): [1, 378, 77, 0.5948568911764831, 211]\n",
      "Best fitness: 83.0\n",
      "Best individual (until now): [1, 378, 77, 0.5948568911764831, 350]\n",
      "Best fitness: 89.0\n",
      "Best individual (until now): [1, 378, 77, 0.5948568911764831, 350]\n",
      "Best fitness: 89.0\n",
      "Best individual (until now): [1, 378, 77, 0.5948568911764831, 350]\n",
      "Best fitness: 89.0\n",
      "Best individual (until now): [1, 378, 77, 0.5948568911764831, 348]\n",
      "Best fitness: 93.0\n",
      "Best individual (until now): [1, 378, 77, 0.5948568911764831, 348]\n",
      "Best fitness: 93.0\n",
      "Best individual (until now): [1, 378, 77, 0.5948568911764831, 348]\n",
      "Best fitness: 93.0\n",
      "Best individual (until now): [1, 378, 77, 0.5948568911764831, 348]\n",
      "Best fitness: 93.0\n",
      "Best individual (until now): [1, 378, 77, 0.5948568911764831, 348]\n",
      "Best fitness: 93.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual (until now): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best fitness: 95.0\n",
      "Best individual with fitness: 95.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 87.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 89.0\n",
      "Best individual (until now): [6, 589, 27, 0.28003986636230477, 409]\n",
      "Best fitness: 89.0\n",
      "Best individual with fitness: 89.0\n"
     ]
    }
   ],
   "source": [
    "BEST_INDIVIDUAL_FIRST = genetic_algorithm(True)\n",
    "BEST_INDIVIDUAL_SECOND = genetic_algorithm(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best individual (starts first turn of the game): [1, 378, 39, 0.5948568911764831, 348]\n",
      "Best individual (starts second turn of the game): [6, 589, 27, 0.28003986636230477, 409]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best individual (starts first turn of the game): {BEST_INDIVIDUAL_FIRST.genotype}\")\n",
    "print(f\"Best individual (starts second turn of the game): {BEST_INDIVIDUAL_SECOND.genotype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
